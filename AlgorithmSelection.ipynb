{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression Algorithm \n",
    "\n",
    "def get_data(url, drop=[]):\n",
    "  import pandas as pd\n",
    "  df = pd.read_csv(url)\n",
    "  if len(drop) > 0:\n",
    "    for col in drop:\n",
    "      df.drop(columns=[col], inplace=True)\n",
    "  return df\n",
    "\n",
    "def bin_groups(df, percent=.05):\n",
    "  import pandas as pd\n",
    "  for col in df:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "      for group, count in df[col].value_counts().iteritems():\n",
    "        if count / len(df) < percent:\n",
    "          df.loc[df[col] == group, col] = 'Other'\n",
    "  return df\n",
    "\n",
    "def drop_columns_missing_50(df, cutoff=.5):\n",
    "  import pandas as pd\n",
    "  for col in df:\n",
    "    if df[col].isna().sum() / len(df) > cutoff:\n",
    "      df.drop(columns=[col], inplace=True)\n",
    "  return df\n",
    "\n",
    "def impute_mean(df):\n",
    "  from sklearn.impute import SimpleImputer\n",
    "  import pandas as pd, numpy as np\n",
    "  for col in df:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "  imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "  return df\n",
    "\n",
    "def fs_variance(df, label=\"\", p=0.8):\n",
    "  from sklearn.feature_selection import VarianceThreshold\n",
    "  import pandas as pd\n",
    "\n",
    "  if label != \"\":\n",
    "    X = df.drop(columns=[label])\n",
    "    \n",
    "  sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
    "  sel.fit_transform(X)\n",
    "\n",
    "  # Add the label back in after removing poor features\n",
    "  return df[sel.get_feature_names_out()].join(df[label])\n",
    "\n",
    "def fit_crossvalidate_mlr(df, k, label, repeat=True):\n",
    "  from sklearn.linear_model import LinearRegression\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  import pandas as pd\n",
    "  from numpy import mean, std\n",
    "  X = df.drop(label,axis=1)\n",
    "  y = df[label]\n",
    "  if repeat:\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=5, random_state=12345)\n",
    "  else:\n",
    "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "  scores = cross_val_score(LinearRegression(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "  print(f'Average R-squared:\\t{mean(scores)}')\n",
    "  return LinearRegression().fit(X, y)\n",
    "\n",
    "def dump_pickle(model, file_name):\n",
    "  import pickle\n",
    "  pickle.dump(model, open(file_name, \"wb\"))\n",
    "\n",
    "def load_pickle(file_name):\n",
    "  import pickle\n",
    "  model = pickle.load(open(file_name, \"rb\"))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average R-squared:\t0.18930810923360947\n"
     ]
    }
   ],
   "source": [
    "#train and test set\n",
    "\n",
    "df = get_data('CuperCut_TrainCrash.csv', [\"CRASH_ID\"])\n",
    "df = bin_groups(df, 0.05)\n",
    "df = drop_columns_missing_50(df)\n",
    "df = impute_mean(df)\n",
    "\n",
    "model = fit_crossvalidate_mlr(df, 10, label='CRASH_SEVERITY_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRASH_ID</th>\n",
       "      <th>MILEPOINT</th>\n",
       "      <th>CRASH_DATETIME_01/01/2016 01:00:00 AM</th>\n",
       "      <th>CRASH_DATETIME_01/01/2016 01:04:00 AM</th>\n",
       "      <th>CRASH_DATETIME_01/01/2016 01:12:00 AM</th>\n",
       "      <th>CRASH_DATETIME_01/01/2016 01:25:00 AM</th>\n",
       "      <th>CRASH_DATETIME_01/01/2016 01:57:00 AM</th>\n",
       "      <th>CRASH_DATETIME_01/01/2016 02:00:00 AM</th>\n",
       "      <th>CRASH_DATETIME_01/01/2016 02:31:00 AM</th>\n",
       "      <th>CRASH_DATETIME_01/01/2016 02:35:00 AM</th>\n",
       "      <th>...</th>\n",
       "      <th>COUNTY_NAME_SUMMIT</th>\n",
       "      <th>COUNTY_NAME_TOOELE</th>\n",
       "      <th>COUNTY_NAME_UINTAH</th>\n",
       "      <th>COUNTY_NAME_UTAH</th>\n",
       "      <th>COUNTY_NAME_WASATCH</th>\n",
       "      <th>COUNTY_NAME_WASHINGTON</th>\n",
       "      <th>COUNTY_NAME_WAYNE</th>\n",
       "      <th>COUNTY_NAME_WEBER</th>\n",
       "      <th>WORK_ZONE_RELATED_False</th>\n",
       "      <th>WORK_ZONE_RELATED_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>10823501</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4880</th>\n",
       "      <td>10835904</td>\n",
       "      <td>2.063000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17786</th>\n",
       "      <td>10915400</td>\n",
       "      <td>6.148000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11580</th>\n",
       "      <td>10893333</td>\n",
       "      <td>46.942987</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>10816864</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63337 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRASH_ID  MILEPOINT  CRASH_DATETIME_01/01/2016 01:00:00 AM  \\\n",
       "1433   10823501   0.100000                                      0   \n",
       "4880   10835904   2.063000                                      0   \n",
       "17786  10915400   6.148000                                      0   \n",
       "11580  10893333  46.942987                                      0   \n",
       "1699   10816864   0.100000                                      0   \n",
       "\n",
       "       CRASH_DATETIME_01/01/2016 01:04:00 AM  \\\n",
       "1433                                       0   \n",
       "4880                                       0   \n",
       "17786                                      0   \n",
       "11580                                      0   \n",
       "1699                                       0   \n",
       "\n",
       "       CRASH_DATETIME_01/01/2016 01:12:00 AM  \\\n",
       "1433                                       0   \n",
       "4880                                       0   \n",
       "17786                                      0   \n",
       "11580                                      0   \n",
       "1699                                       0   \n",
       "\n",
       "       CRASH_DATETIME_01/01/2016 01:25:00 AM  \\\n",
       "1433                                       0   \n",
       "4880                                       0   \n",
       "17786                                      0   \n",
       "11580                                      0   \n",
       "1699                                       0   \n",
       "\n",
       "       CRASH_DATETIME_01/01/2016 01:57:00 AM  \\\n",
       "1433                                       0   \n",
       "4880                                       0   \n",
       "17786                                      0   \n",
       "11580                                      0   \n",
       "1699                                       0   \n",
       "\n",
       "       CRASH_DATETIME_01/01/2016 02:00:00 AM  \\\n",
       "1433                                       0   \n",
       "4880                                       0   \n",
       "17786                                      0   \n",
       "11580                                      0   \n",
       "1699                                       0   \n",
       "\n",
       "       CRASH_DATETIME_01/01/2016 02:31:00 AM  \\\n",
       "1433                                       0   \n",
       "4880                                       0   \n",
       "17786                                      0   \n",
       "11580                                      0   \n",
       "1699                                       0   \n",
       "\n",
       "       CRASH_DATETIME_01/01/2016 02:35:00 AM  ...  COUNTY_NAME_SUMMIT  \\\n",
       "1433                                       0  ...                   0   \n",
       "4880                                       0  ...                   0   \n",
       "17786                                      0  ...                   0   \n",
       "11580                                      0  ...                   0   \n",
       "1699                                       0  ...                   0   \n",
       "\n",
       "       COUNTY_NAME_TOOELE  COUNTY_NAME_UINTAH  COUNTY_NAME_UTAH  \\\n",
       "1433                    0                   0                 0   \n",
       "4880                    0                   0                 0   \n",
       "17786                   0                   0                 0   \n",
       "11580                   0                   0                 0   \n",
       "1699                    0                   0                 0   \n",
       "\n",
       "       COUNTY_NAME_WASATCH  COUNTY_NAME_WASHINGTON  COUNTY_NAME_WAYNE  \\\n",
       "1433                     0                       0                  0   \n",
       "4880                     0                       0                  0   \n",
       "17786                    0                       0                  0   \n",
       "11580                    0                       0                  0   \n",
       "1699                     0                       0                  0   \n",
       "\n",
       "       COUNTY_NAME_WEBER  WORK_ZONE_RELATED_False  WORK_ZONE_RELATED_True  \n",
       "1433                   0                        1                       0  \n",
       "4880                   0                        1                       0  \n",
       "17786                  0                        1                       0  \n",
       "11580                  0                        1                       0  \n",
       "1699                   0                        1                       0  \n",
       "\n",
       "[5 rows x 63337 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('CuperCut_TrainCrash.csv')\n",
    "\n",
    "for col in df:\n",
    "  if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "    df = df.join(pd.get_dummies(df[col], prefix=col))\n",
    "\n",
    "y = df['CRASH_SEVERITY_ID']\n",
    "X = df.drop(columns=[\"CRASH_SEVERITY_ID\"])\n",
    "X = X.select_dtypes(np.number)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17563\n",
      "1951\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average R-squared:\t0.18930810923360947\n"
     ]
    }
   ],
   "source": [
    "x_test = get_data('CuperCut_TrainCrash.csv', ['CRASH_ID'])\n",
    "x_test = bin_groups(x_test, 0.05)\n",
    "x_test = drop_columns_missing_50(x_test)\n",
    "x_test = impute_mean(x_test)\n",
    "\n",
    "\n",
    "model = fit_crossvalidate_mlr(x_test, 10, label='CRASH_SEVERITY_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19514\n"
     ]
    }
   ],
   "source": [
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF</th>\n",
       "      <th>Tolerance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ROUTE_Other</th>\n",
       "      <td>6.407352</td>\n",
       "      <td>0.156071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SINGLE_VEHICLE</th>\n",
       "      <td>4.271322</td>\n",
       "      <td>0.234120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAIN_ROAD_NAME_Other</th>\n",
       "      <td>4.220854</td>\n",
       "      <td>0.236919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MILEPOINT</th>\n",
       "      <td>3.998592</td>\n",
       "      <td>0.250088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTY_NAME_SALT LAKE</th>\n",
       "      <td>3.490743</td>\n",
       "      <td>0.286472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROADWAY_DEPARTURE</th>\n",
       "      <td>3.193965</td>\n",
       "      <td>0.313091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTY_NAME_Other</th>\n",
       "      <td>3.142115</td>\n",
       "      <td>0.318257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUTE_89</th>\n",
       "      <td>3.028699</td>\n",
       "      <td>0.330175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITY_Other</th>\n",
       "      <td>2.535910</td>\n",
       "      <td>0.394336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTY_NAME_UTAH</th>\n",
       "      <td>2.232440</td>\n",
       "      <td>0.447940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WILD_ANIMAL_RELATED</th>\n",
       "      <td>2.073901</td>\n",
       "      <td>0.482183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITY_SALT LAKE CITY</th>\n",
       "      <td>1.889176</td>\n",
       "      <td>0.529331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITY_WEST VALLEY CITY</th>\n",
       "      <td>1.807008</td>\n",
       "      <td>0.553401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTY_NAME_WEBER</th>\n",
       "      <td>1.651496</td>\n",
       "      <td>0.605511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTERSECTION_RELATED</th>\n",
       "      <td>1.310119</td>\n",
       "      <td>0.763289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OVERTURN_ROLLOVER</th>\n",
       "      <td>1.272236</td>\n",
       "      <td>0.786017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEDESTRIAN_INVOLVED</th>\n",
       "      <td>1.250334</td>\n",
       "      <td>0.799786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRASH_SEVERITY_ID</th>\n",
       "      <td>1.240214</td>\n",
       "      <td>0.806313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BICYCLIST_INVOLVED</th>\n",
       "      <td>1.225035</td>\n",
       "      <td>0.816303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NIGHT_DARK_CONDITION</th>\n",
       "      <td>1.136134</td>\n",
       "      <td>0.880178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            VIF  Tolerance\n",
       "ROUTE_Other            6.407352   0.156071\n",
       "SINGLE_VEHICLE         4.271322   0.234120\n",
       "MAIN_ROAD_NAME_Other   4.220854   0.236919\n",
       "MILEPOINT              3.998592   0.250088\n",
       "COUNTY_NAME_SALT LAKE  3.490743   0.286472\n",
       "ROADWAY_DEPARTURE      3.193965   0.313091\n",
       "COUNTY_NAME_Other      3.142115   0.318257\n",
       "ROUTE_89               3.028699   0.330175\n",
       "CITY_Other             2.535910   0.394336\n",
       "COUNTY_NAME_UTAH       2.232440   0.447940\n",
       "WILD_ANIMAL_RELATED    2.073901   0.482183\n",
       "CITY_SALT LAKE CITY    1.889176   0.529331\n",
       "CITY_WEST VALLEY CITY  1.807008   0.553401\n",
       "COUNTY_NAME_WEBER      1.651496   0.605511\n",
       "INTERSECTION_RELATED   1.310119   0.763289\n",
       "OVERTURN_ROLLOVER      1.272236   0.786017\n",
       "PEDESTRIAN_INVOLVED    1.250334   0.799786\n",
       "CRASH_SEVERITY_ID      1.240214   0.806313\n",
       "BICYCLIST_INVOLVED     1.225035   0.816303\n",
       "NIGHT_DARK_CONDITION   1.136134   0.880178"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does it mean to get a negative R-squared value? https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#:~:text=(coefficient%20of%20determination)%20regression%20score,get%20a%20score%20of%200.0.\n",
    "# Why is our model so much worse than random? Is there a pattern between the important features?\n",
    "# MEME time\n",
    "\n",
    "def vif(df, label=\"\"):\n",
    "  import pandas as pd\n",
    "  from sklearn.linear_model import LinearRegression\n",
    "  \n",
    "  # initialize dictionaries\n",
    "  vif_dict, tolerance_dict = {}, {}\n",
    "\n",
    "  # drop unnecessary columns if they are found in the dataframe\n",
    "  if label in df.columns: df.drop(columns=[label], inplace=True)\n",
    "  if 'const' in df.columns: df.drop(columns=['const'], inplace=True)\n",
    "\n",
    "  # form input data for each exogenous variable\n",
    "  for col in df:\n",
    "    y = df[col]\n",
    "    X = df.drop(columns=[col])\n",
    "    \n",
    "    # extract r-squared from the fit\n",
    "    r_squared = LinearRegression().fit(X, y).score(X, y)\n",
    "\n",
    "    # calculate VIF\n",
    "    if r_squared < 1: # Prevent division by zero runtime error\n",
    "      vif = 1/(1 - r_squared) \n",
    "    else:\n",
    "      vif = 100\n",
    "    vif_dict[col] = vif\n",
    "\n",
    "    # calculate tolerance\n",
    "    tolerance = 1 - r_squared\n",
    "    tolerance_dict[col] = tolerance\n",
    "\n",
    "  # generate the DataFrame to return\n",
    "  return pd.DataFrame({'VIF': vif_dict, 'Tolerance': tolerance_dict}).sort_values(by=['VIF'], ascending=False)\n",
    "\n",
    "vif(x_test).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree R-squared:\t-0.5408340276487364\n"
     ]
    }
   ],
   "source": [
    "# Because of the multicollinearity issues we're seeing with our data, let's fit a decision tree instead of MLR\n",
    "\n",
    "# Multivariate feature importance for tree model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "from numpy import mean\n",
    "\n",
    "y = x_test.CRASH_SEVERITY_ID\n",
    "X = x_test.drop(columns=['CRASH_SEVERITY_ID'])\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=12345)\n",
    "scores = cross_val_score(DecisionTreeRegressor(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "print(f'Decision Tree R-squared:\\t{mean(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MILEPOINT</th>\n",
       "      <td>0.511498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEDESTRIAN_INVOLVED</th>\n",
       "      <td>0.039664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BICYCLIST_INVOLVED</th>\n",
       "      <td>0.022246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOTORCYCLE_INVOLVED</th>\n",
       "      <td>0.055615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMPROPER_RESTRAINT</th>\n",
       "      <td>0.008152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNRESTRAINED</th>\n",
       "      <td>0.015195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUI</th>\n",
       "      <td>0.011498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTERSECTION_RELATED</th>\n",
       "      <td>0.013694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WILD_ANIMAL_RELATED</th>\n",
       "      <td>0.004807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOMESTIC_ANIMAL_RELATED</th>\n",
       "      <td>0.000966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OVERTURN_ROLLOVER</th>\n",
       "      <td>0.040721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMMERCIAL_MOTOR_VEH_INVOLVED</th>\n",
       "      <td>0.017136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEENAGE_DRIVER_INVOLVED</th>\n",
       "      <td>0.033439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLDER_DRIVER_INVOLVED</th>\n",
       "      <td>0.015985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NIGHT_DARK_CONDITION</th>\n",
       "      <td>0.034262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SINGLE_VEHICLE</th>\n",
       "      <td>0.012014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISTRACTED_DRIVING</th>\n",
       "      <td>0.014818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DROWSY_DRIVING</th>\n",
       "      <td>0.006523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROADWAY_DEPARTURE</th>\n",
       "      <td>0.017147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUTE_89</th>\n",
       "      <td>0.003586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUTE_Other</th>\n",
       "      <td>0.007535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAIN_ROAD_NAME_Other</th>\n",
       "      <td>0.009821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITY_Other</th>\n",
       "      <td>0.020816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITY_SALT LAKE CITY</th>\n",
       "      <td>0.006572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITY_WEST VALLEY CITY</th>\n",
       "      <td>0.007853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTY_NAME_Other</th>\n",
       "      <td>0.018321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTY_NAME_SALT LAKE</th>\n",
       "      <td>0.015031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTY_NAME_UTAH</th>\n",
       "      <td>0.013230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTY_NAME_WEBER</th>\n",
       "      <td>0.009296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WORK_ZONE_RELATED_Other</th>\n",
       "      <td>0.012560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Feature Importance\n",
       "MILEPOINT                                0.511498\n",
       "PEDESTRIAN_INVOLVED                      0.039664\n",
       "BICYCLIST_INVOLVED                       0.022246\n",
       "MOTORCYCLE_INVOLVED                      0.055615\n",
       "IMPROPER_RESTRAINT                       0.008152\n",
       "UNRESTRAINED                             0.015195\n",
       "DUI                                      0.011498\n",
       "INTERSECTION_RELATED                     0.013694\n",
       "WILD_ANIMAL_RELATED                      0.004807\n",
       "DOMESTIC_ANIMAL_RELATED                  0.000966\n",
       "OVERTURN_ROLLOVER                        0.040721\n",
       "COMMERCIAL_MOTOR_VEH_INVOLVED            0.017136\n",
       "TEENAGE_DRIVER_INVOLVED                  0.033439\n",
       "OLDER_DRIVER_INVOLVED                    0.015985\n",
       "NIGHT_DARK_CONDITION                     0.034262\n",
       "SINGLE_VEHICLE                           0.012014\n",
       "DISTRACTED_DRIVING                       0.014818\n",
       "DROWSY_DRIVING                           0.006523\n",
       "ROADWAY_DEPARTURE                        0.017147\n",
       "ROUTE_89                                 0.003586\n",
       "ROUTE_Other                              0.007535\n",
       "MAIN_ROAD_NAME_Other                     0.009821\n",
       "CITY_Other                               0.020816\n",
       "CITY_SALT LAKE CITY                      0.006572\n",
       "CITY_WEST VALLEY CITY                    0.007853\n",
       "COUNTY_NAME_Other                        0.018321\n",
       "COUNTY_NAME_SALT LAKE                    0.015031\n",
       "COUNTY_NAME_UTAH                         0.013230\n",
       "COUNTY_NAME_WEBER                        0.009296\n",
       "WORK_ZONE_RELATED_Other                  0.012560"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fi = pd.DataFrame({\"Feature Importance\":model.feature_importances_}, index=x_test.drop(columns=['CRASH_SEVERITY_ID']).columns)\n",
    "df_fi.sort_values(by=['Feature Importance'], ascending=False).head(20)\n",
    "\n",
    "df_fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.188899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.188888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.019274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          R-squared\n",
       "Ridge      0.188900\n",
       "OLS        0.188899\n",
       "Bayesian   0.188888\n",
       "Poisson    0.019274\n",
       "Lasso      0.000182\n",
       "Gamma     -0.000095\n",
       "Inverse   -0.000095\n",
       "LARS      -0.000095"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at Linear Models (top-down) and test many of the Linear Models (bottom-up) to see which model produces the best fit metrics\n",
    "# Scikit-learn documenation: https://scikit-learn.org/stable/modules/linear_model.html#linear-model\n",
    "\n",
    "fit = {}    # Use this to store each of the fit metrics\n",
    "models = {} # Use this to store each of the models\n",
    "        \n",
    "# 1. LINEAR MODELS: Assumes normal distribution, homoscedasticity, no multicollinearity, independence, and no auto-correlation (some exceptions apply; some of these algorithms are better at handling violations of these assumptions).\n",
    "import sklearn.linear_model as lm, pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from numpy import mean\n",
    "\n",
    "# We will automate this later, just create\n",
    "X = x_test.drop(columns=['CRASH_SEVERITY_ID'])\n",
    "y = x_test['CRASH_SEVERITY_ID']\n",
    "\n",
    "# Set up a standard cross_validation object to use for each algorithm\n",
    "cv = KFold(n_splits=5, random_state=12345, shuffle=True)\n",
    "\n",
    "# 1.1. Ordinary Least Squares Multiple Linear Regression\n",
    "model_ols = lm.LinearRegression()\n",
    "fit['OLS'] = mean(cross_val_score(model_ols, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['OLS'] = model_ols\n",
    "\n",
    "# 1.2. Ridge Regression: More robust to multicollinearity\n",
    "model_rr = lm.Ridge(alpha=0.5) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "fit['Ridge'] = mean(cross_val_score(model_rr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Ridge'] = model_rr\n",
    "\n",
    "# 1.3. Lasso Regression: Better for sparse values like RetweetCount where most are zeros but a few have many retweets.\n",
    "model_lr = lm.Lasso(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "fit['Lasso'] = mean(cross_val_score(model_lr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Lasso'] = model_lr\n",
    "\n",
    "# 1.4. Least Angle Regression: Good when the number of features is greater than the number of samples.\n",
    "model_llr = lm.LassoLars(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "fit['LARS'] = mean(cross_val_score(model_llr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['LARS'] = model_llr\n",
    "\n",
    "# 1.5. Bayesian Regression: Probability based and allows regularization parameters, automatically tuned to data.\n",
    "model_br = lm.BayesianRidge()\n",
    "fit['Bayesian'] = mean(cross_val_score(model_br, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Bayesian'] = model_br\n",
    "\n",
    "# 1.6. Generalized Linear Regression (Poisson): Good for non-normal distribution, count-based data, and a Poisson distribution.\n",
    "model_pr = lm.TweedieRegressor(power=1, link=\"log\") # Power=1 means this is a Poisson\n",
    "fit['Poisson'] = mean(cross_val_score(model_pr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Poisson'] = model_pr\n",
    "\n",
    "# 1.7. Generalized Linear Regression (Gamma): Good for non-normal distribution, continuous data, and a Gamma distribution.\n",
    "model_gr = lm.TweedieRegressor(power=2, link=\"log\") # Power=2 means this is a Gamma\n",
    "fit['Gamma'] = mean(cross_val_score(model_gr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Gamma'] = model_gr\n",
    "\n",
    "# 1.8. Generalized Linear Regression (Inverse Gamma): Good non-normal distribution, continuous data, and an inverse Gamma distribution.\n",
    "model_igr = lm.TweedieRegressor(power=3) # Power=3 means this is an inverse Gamma\n",
    "fit['Inverse'] = mean(cross_val_score(model_igr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Inverse'] = model_igr\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.188899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.188888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.019274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>-0.004918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>-0.062214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>-0.186458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             R-squared\n",
       "Ridge         0.188900\n",
       "OLS           0.188899\n",
       "Bayesian      0.188888\n",
       "Poisson       0.019274\n",
       "Lasso         0.000182\n",
       "Gamma        -0.000095\n",
       "Inverse      -0.000095\n",
       "LARS         -0.000095\n",
       "NuSupportVM  -0.004918\n",
       "Linear SVM   -0.062214\n",
       "SupportVM    -0.186458"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#support vector machines futhers the research of which model looks better\n",
    "# Let's continue to test another family of regression algorithms: Support Vector Machines\n",
    "\n",
    "# SUPPORT VECTOR MACHINES: Ideal for noisy data with large gaps among values\n",
    "from sklearn import svm\n",
    "\n",
    "# 1.9. SVM: this is the default SVM, parameters can be modified to make this more accurate\n",
    "model_svm = svm.SVR()\n",
    "fit['SupportVM'] = mean(cross_val_score(model_svm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['SupportVM'] = model_svm\n",
    "\n",
    "# 1.10. Linear SVM: Faster than SVM but only considers a linear model\n",
    "model_lsvm = svm.LinearSVR()\n",
    "fit['Linear SVM'] = mean(cross_val_score(model_lsvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Linear SVM'] = model_lsvm\n",
    "\n",
    "# 1.11. NuSVM: \n",
    "model_nusvm = svm.NuSVR()\n",
    "fit['NuSupportVM'] = mean(cross_val_score(model_nusvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['NuSupportVM'] = model_nusvm\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)\n",
    "\n",
    "#The Ridge is still the best set of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.188899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.188888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.019274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.017739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>-0.004918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>-0.062214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighborsD</th>\n",
       "      <td>-0.065430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>-0.186458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              R-squared\n",
       "Ridge          0.188900\n",
       "OLS            0.188899\n",
       "Bayesian       0.188888\n",
       "Poisson        0.019274\n",
       "KNNeighbors    0.017739\n",
       "Lasso          0.000182\n",
       "Gamma         -0.000095\n",
       "Inverse       -0.000095\n",
       "LARS          -0.000095\n",
       "NuSupportVM   -0.004918\n",
       "Linear SVM    -0.062214\n",
       "KNNeighborsD  -0.065430\n",
       "SupportVM     -0.186458"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we will try the k-nearest neighbors (KNN) algorithm, which can be used for both labeled \n",
    "# and unlabeled data as well as both regression and classification problems\n",
    "\n",
    "# KNN: NEAREST NEIGHBORS REGRESSION\n",
    "from sklearn import neighbors\n",
    "\n",
    "# 1.12. KNeighborsRegressor: \n",
    "model_knnr = neighbors.KNeighborsRegressor(n_neighbors=10, weights='uniform')\n",
    "fit['KNNeighbors'] = mean(cross_val_score(model_knnr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['KNNeighbors'] = model_knnr\n",
    "\n",
    "# 1.13. KNeighborsRegressor: \n",
    "model_knnrd = neighbors.KNeighborsRegressor(n_neighbors=10, weights='distance')\n",
    "fit['KNNeighborsD'] = mean(cross_val_score(model_knnrd, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['KNNeighborsD'] = model_knnrd\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 272, in fit\n",
      "    self._constrained_optimization(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 603, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 681, in minimize\n",
      "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 308, in _minimize_lbfgsb\n",
      "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 263, in _prepare_scalar_function\n",
      "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 158, in __init__\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 251, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 155, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 137, in fun_wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 76, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 70, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 262, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 527, in log_marginal_likelihood\n",
      "    K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 843, in __call__\n",
      "    K2, K2_gradient = self.k2(X, Y, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 1382, in __call__\n",
      "    K = self.noise_level * np.eye(_num_samples(X))\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.82 GiB for an array with shape (15611, 15611) and data type float64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 272, in fit\n",
      "    self._constrained_optimization(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 603, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 681, in minimize\n",
      "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 308, in _minimize_lbfgsb\n",
      "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 263, in _prepare_scalar_function\n",
      "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 158, in __init__\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 251, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 155, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 137, in fun_wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 76, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 70, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 262, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 527, in log_marginal_likelihood\n",
      "    K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 843, in __call__\n",
      "    K2, K2_gradient = self.k2(X, Y, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 1382, in __call__\n",
      "    K = self.noise_level * np.eye(_num_samples(X))\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.82 GiB for an array with shape (15612, 15612) and data type float64\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.188899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.188888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.019274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.017739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>-0.004918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>-0.062214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighborsD</th>\n",
       "      <td>-0.065430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>-0.186458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianP</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              R-squared\n",
       "Ridge          0.188900\n",
       "OLS            0.188899\n",
       "Bayesian       0.188888\n",
       "Poisson        0.019274\n",
       "KNNeighbors    0.017739\n",
       "Lasso          0.000182\n",
       "Gamma         -0.000095\n",
       "Inverse       -0.000095\n",
       "LARS          -0.000095\n",
       "NuSupportVM   -0.004918\n",
       "Linear SVM    -0.062214\n",
       "KNNeighborsD  -0.065430\n",
       "SupportVM     -0.186458\n",
       "GaussianP           NaN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, letâ€™s try a single algorithm called a Gaussian process regression (GPR). \n",
    "# GPR has some great benefits, including more accuracy with small datasets\n",
    "\n",
    "# GAUSSIAN PROCESS REGRESSION\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "# 1.14. GaussianProcessRegressor:\n",
    "model_gpr = gaussian_process.GaussianProcessRegressor(DotProduct() + WhiteKernel())\n",
    "fit['GaussianP'] = mean(cross_val_score(model_gpr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['GaussianP'] = model_gpr\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HG Boost</th>\n",
       "      <td>0.196775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grad. Boost</th>\n",
       "      <td>0.195872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.188899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.188888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacking</th>\n",
       "      <td>0.178878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting</th>\n",
       "      <td>0.079599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Forest</th>\n",
       "      <td>0.036783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.019274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.017739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>-0.004918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>-0.062214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighborsD</th>\n",
       "      <td>-0.065430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost DT</th>\n",
       "      <td>-0.100454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>-0.186458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>-0.297127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Tree</th>\n",
       "      <td>-0.532550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianP</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              R-squared\n",
       "HG Boost       0.196775\n",
       "Grad. Boost    0.195872\n",
       "Ridge          0.188900\n",
       "OLS            0.188899\n",
       "Bayesian       0.188888\n",
       "Stacking       0.178878\n",
       "Voting         0.079599\n",
       "Dec Forest     0.036783\n",
       "Poisson        0.019274\n",
       "KNNeighbors    0.017739\n",
       "Lasso          0.000182\n",
       "Gamma         -0.000095\n",
       "Inverse       -0.000095\n",
       "LARS          -0.000095\n",
       "NuSupportVM   -0.004918\n",
       "Linear SVM    -0.062214\n",
       "KNNeighborsD  -0.065430\n",
       "AdaBoost DT   -0.100454\n",
       "SupportVM     -0.186458\n",
       "Extra Trees   -0.297127\n",
       "Dec Tree      -0.532550\n",
       "GaussianP           NaN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DECISION TREE MODELS: no assumptions about the data\n",
    "import sklearn.tree as tree\n",
    "import sklearn.ensemble as se\n",
    "\n",
    "# 1.15. Decision Tree Regression\n",
    "model_dt = tree.DecisionTreeRegressor(random_state=12345)\n",
    "fit['Dec Tree'] = mean(cross_val_score(model_dt, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Dec Tree'] = model_dt\n",
    "\n",
    "\n",
    "# DECISION TREE-BASED ENSEMBLE MODELS: great for minimizing overfitting, these are based on averaging many unique sub-samples and combining algorithms \n",
    "# 1.16. Decision Forrest\n",
    "model_df = se.RandomForestRegressor(random_state=12345)\n",
    "fit['Dec Forest'] = mean(cross_val_score(model_df, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Dec Forest'] = model_df\n",
    "\n",
    "# 1.17. ExtraTreesRegressor\n",
    "model_etr = se.ExtraTreesRegressor(random_state=12345)\n",
    "fit['Extra Trees'] = mean(cross_val_score(model_etr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Extra Trees'] = model_etr\n",
    "\n",
    "# 1.18. AdaBoostRegressor\n",
    "model_abr = se.AdaBoostRegressor(n_estimators=100, random_state=12345)\n",
    "fit['AdaBoost DT'] = mean(cross_val_score(model_abr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['AdaBoost DT'] = model_abr\n",
    "\n",
    "# 1.19. GradientBoostingRegressor\n",
    "model_gbr = se.GradientBoostingRegressor(random_state=12345)\n",
    "fit['Grad. Boost'] = mean(cross_val_score(model_gbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Grad. Boost'] = model_gbr\n",
    "\n",
    "# 1.20. HistGradientBoostingRegressor\n",
    "model_hgbr = se.HistGradientBoostingRegressor(random_state=12345)\n",
    "fit['HG Boost'] = mean(cross_val_score(model_hgbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['HG Boost'] = model_hgbr\n",
    "\n",
    "# 1.21. VotingRegressor: will combine other algorithms into an average; kind of cool\n",
    "model_vr = se.VotingRegressor(estimators=[('DT', model_dt), ('DF', model_df), ('ETR', model_etr), ('ABR', model_abr), ('GBR', model_gbr)])\n",
    "fit['Voting'] = mean(cross_val_score(model_vr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Voting'] = model_vr\n",
    "\n",
    "# 1.22. StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "estimators = [('ridge', RidgeCV()), ('lasso', LassoCV(random_state=42)), ('svr', svm.SVR(C=1, gamma=1e-6))]\n",
    "model_sr = se.StackingRegressor(estimators=estimators, final_estimator=se.GradientBoostingRegressor(random_state=12345))\n",
    "fit['Stacking'] = mean(cross_val_score(model_sr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Stacking'] = model_sr\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HG Boost</th>\n",
       "      <td>0.196775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grad. Boost</th>\n",
       "      <td>0.195872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.188899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.188888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacking</th>\n",
       "      <td>0.178878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting</th>\n",
       "      <td>0.079599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Forest</th>\n",
       "      <td>0.036783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.035160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.019274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.017739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>-0.004918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>-0.062214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighborsD</th>\n",
       "      <td>-0.065430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost DT</th>\n",
       "      <td>-0.100454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>-0.186458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>-0.297127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Tree</th>\n",
       "      <td>-0.532550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianP</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              R-squared\n",
       "HG Boost       0.196775\n",
       "Grad. Boost    0.195872\n",
       "Ridge          0.188900\n",
       "OLS            0.188899\n",
       "Bayesian       0.188888\n",
       "Stacking       0.178878\n",
       "Voting         0.079599\n",
       "Dec Forest     0.036783\n",
       "XGBoost        0.035160\n",
       "Poisson        0.019274\n",
       "KNNeighbors    0.017739\n",
       "Lasso          0.000182\n",
       "Gamma         -0.000095\n",
       "Inverse       -0.000095\n",
       "LARS          -0.000095\n",
       "NuSupportVM   -0.004918\n",
       "Linear SVM    -0.062214\n",
       "KNNeighborsD  -0.065430\n",
       "AdaBoost DT   -0.100454\n",
       "SupportVM     -0.186458\n",
       "Extra Trees   -0.297127\n",
       "Dec Tree      -0.532550\n",
       "GaussianP           NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we just tried a gradient boosted model and had great success, it is worth taking a slight pause from the scikit-learn package \n",
    "# to add an algorithm from a popular package known as XGBoost. XGBoost is built on top of the scikit-learn package and works much the same way. \n",
    "# It is popular because it offers a faster and often slightly more accurate version of a gradient boosting algorithm.\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1.23. XGBRegressor\n",
    "model_xgb = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['XGBoost'] = model_xgb\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HG Boost</th>\n",
       "      <td>0.196775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grad. Boost</th>\n",
       "      <td>0.195872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.188899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.188888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacking</th>\n",
       "      <td>0.178878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting</th>\n",
       "      <td>0.079599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Forest</th>\n",
       "      <td>0.036783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.035160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.019274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.017739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeuralNet</th>\n",
       "      <td>0.001779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>-0.004918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>-0.062214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighborsD</th>\n",
       "      <td>-0.065430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost DT</th>\n",
       "      <td>-0.100454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>-0.186458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>-0.297127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Tree</th>\n",
       "      <td>-0.532550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianP</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              R-squared\n",
       "HG Boost       0.196775\n",
       "Grad. Boost    0.195872\n",
       "Ridge          0.188900\n",
       "OLS            0.188899\n",
       "Bayesian       0.188888\n",
       "Stacking       0.178878\n",
       "Voting         0.079599\n",
       "Dec Forest     0.036783\n",
       "XGBoost        0.035160\n",
       "Poisson        0.019274\n",
       "KNNeighbors    0.017739\n",
       "NeuralNet      0.001779\n",
       "Lasso          0.000182\n",
       "Gamma         -0.000095\n",
       "Inverse       -0.000095\n",
       "LARS          -0.000095\n",
       "NuSupportVM   -0.004918\n",
       "Linear SVM    -0.062214\n",
       "KNNeighborsD  -0.065430\n",
       "AdaBoost DT   -0.100454\n",
       "SupportVM     -0.186458\n",
       "Extra Trees   -0.297127\n",
       "Dec Tree      -0.532550\n",
       "GaussianP           NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEURAL-NETWORK MODELS: Based on deep learning methods\n",
    "import sklearn.neural_network as nn\n",
    "\n",
    "# 1.23. MLPRegressor\n",
    "model_nn = nn.MLPRegressor(max_iter=1000, random_state=12345) # Turn max_iter way up or down to get a more accurate result\n",
    "fit['NeuralNet'] = mean(cross_val_score(model_nn, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['NeuralNet'] = model_nn\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a few minutes to create a function that will automate the process that we just went through. It should\n",
    "# (1) Try every algorithm and \n",
    "# (2) Automatically select the best one\n",
    "# Do this with the first three algorithms first, then we can expand it to include all algorithms\n",
    "# This function will replace fit_crossvalidate_mlr() in our pipeline (so that function is a good starting point)\n",
    "\n",
    "def fit_crossvalidate_reg(df, label, k=10, r=5, repeat=True):\n",
    "  import sklearn.linear_model as lm\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  import pandas as pd\n",
    "  from numpy import mean, std\n",
    "  X = df.drop(label,axis=1)\n",
    "  y = df[label]\n",
    "  if repeat:\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=12345)\n",
    "  else:\n",
    "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "\n",
    "  fit = {}\n",
    "  model = {}\n",
    "\n",
    "  # Create the model objects\n",
    "  \n",
    "  model_ols = lm.LinearRegression()\n",
    "  model_rr = lm.Ridge(alpha=0.5)\n",
    "  model_lr = lm.Lasso(alpha=0.1)\n",
    "\n",
    "  # Fit a cross-validated R squared score and add it to the dict\n",
    "  \n",
    "  fit['OLS'] = mean(cross_val_score(model_ols, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Ridge'] = mean(cross_val_score(model_rr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Lasso'] = mean(cross_val_score(model_lr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "\n",
    "  # Add the model to another dict; make sure the keys have the same names as the list above  \n",
    "\n",
    "  models['OLS'] = model_ols\n",
    "  models['Ridge'] = model_rr\n",
    "  models['Lasso'] = model_lr\n",
    "\n",
    "  df_fit = pd.DataFrame({'R-squared':fit})\n",
    "  df_fit.sort_values(by=['R-squared'], ascending=False, inplace=True)\n",
    "  best_model = df_fit.index[0]\n",
    "  print(df_fit)\n",
    "\n",
    "  return models[best_model].fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       R-squared\n",
      "Ridge  -0.000343\n",
      "OLS    -0.000343\n",
      "Lasso  -0.000344\n"
     ]
    }
   ],
   "source": [
    "df = get_data('CuperCut_TrainCrash.csv', ['CRASH_ID'])\n",
    "df = bin_groups(df)\n",
    "df = drop_columns_missing_50(df)\n",
    "df = impute_mean(df)\n",
    "\n",
    "# Feature selection and modeling pipeline\n",
    "df = fs_variance(df, label=\"CRASH_SEVERITY_ID\", p=.5)\n",
    "model = fit_crossvalidate_reg(df, 'CRASH_SEVERITY_ID', 5, 2)\n",
    "\n",
    "# Deploy/store the model\n",
    "dump_pickle(model, 'best_reg_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 10.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 272, in fit\n",
      "    self._constrained_optimization(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 603, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 681, in minimize\n",
      "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 308, in _minimize_lbfgsb\n",
      "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 263, in _prepare_scalar_function\n",
      "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 158, in __init__\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 251, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 155, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 137, in fun_wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 76, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 70, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 262, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 527, in log_marginal_likelihood\n",
      "    K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 842, in __call__\n",
      "    K1, K1_gradient = self.k1(X, Y, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 2169, in __call__\n",
      "    K = np.inner(X, X) + self.sigma_0 ** 2\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.82 GiB for an array with shape (15611, 15611) and data type float64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 272, in fit\n",
      "    self._constrained_optimization(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 603, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 681, in minimize\n",
      "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 308, in _minimize_lbfgsb\n",
      "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 263, in _prepare_scalar_function\n",
      "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 158, in __init__\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 251, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 155, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 137, in fun_wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 76, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 70, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 262, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 527, in log_marginal_likelihood\n",
      "    K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 843, in __call__\n",
      "    K2, K2_gradient = self.k2(X, Y, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 1382, in __call__\n",
      "    K = self.noise_level * np.eye(_num_samples(X))\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.82 GiB for an array with shape (15611, 15611) and data type float64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 272, in fit\n",
      "    self._constrained_optimization(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 603, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 681, in minimize\n",
      "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 308, in _minimize_lbfgsb\n",
      "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 263, in _prepare_scalar_function\n",
      "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 158, in __init__\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 251, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 155, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 137, in fun_wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 76, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 70, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 262, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 527, in log_marginal_likelihood\n",
      "    K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 843, in __call__\n",
      "    K2, K2_gradient = self.k2(X, Y, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 1382, in __call__\n",
      "    K = self.noise_level * np.eye(_num_samples(X))\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\twodim_base.py\", line 214, in eye\n",
      "    m = zeros((N, M), dtype=dtype, order=order)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.82 GiB for an array with shape (15612, 15612) and data type float64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 272, in fit\n",
      "    self._constrained_optimization(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 603, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 681, in minimize\n",
      "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 308, in _minimize_lbfgsb\n",
      "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 263, in _prepare_scalar_function\n",
      "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 158, in __init__\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 251, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 155, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 137, in fun_wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 76, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 70, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 262, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 527, in log_marginal_likelihood\n",
      "    K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 843, in __call__\n",
      "    K2, K2_gradient = self.k2(X, Y, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 1382, in __call__\n",
      "    K = self.noise_level * np.eye(_num_samples(X))\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\twodim_base.py\", line 214, in eye\n",
      "    m = zeros((N, M), dtype=dtype, order=order)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.82 GiB for an array with shape (15611, 15611) and data type float64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 272, in fit\n",
      "    self._constrained_optimization(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 603, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 681, in minimize\n",
      "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 308, in _minimize_lbfgsb\n",
      "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 263, in _prepare_scalar_function\n",
      "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 158, in __init__\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 251, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 155, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 137, in fun_wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 76, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 70, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 262, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 527, in log_marginal_likelihood\n",
      "    K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 844, in __call__\n",
      "    return K1 + K2, np.dstack((K1_gradient, K2_gradient))\n",
      "  File \"<__array_function__ internals>\", line 180, in dstack\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\shape_base.py\", line 723, in dstack\n",
      "    return _nx.concatenate(arrs, 2)\n",
      "  File \"<__array_function__ internals>\", line 180, in concatenate\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 3.63 GiB for an array with shape (15611, 15611, 2) and data type float64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 272, in fit\n",
      "    self._constrained_optimization(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 603, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 681, in minimize\n",
      "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 308, in _minimize_lbfgsb\n",
      "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 263, in _prepare_scalar_function\n",
      "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 158, in __init__\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 251, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 155, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 137, in fun_wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 76, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 70, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 262, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 527, in log_marginal_likelihood\n",
      "    K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 842, in __call__\n",
      "    K1, K1_gradient = self.k1(X, Y, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 2169, in __call__\n",
      "    K = np.inner(X, X) + self.sigma_0 ** 2\n",
      "  File \"<__array_function__ internals>\", line 180, in inner\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.82 GiB for an array with shape (15611, 15611) and data type float64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 272, in fit\n",
      "    self._constrained_optimization(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 603, in _constrained_optimization\n",
      "    opt_res = scipy.optimize.minimize(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 681, in minimize\n",
      "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 308, in _minimize_lbfgsb\n",
      "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 263, in _prepare_scalar_function\n",
      "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 158, in __init__\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 251, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 155, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 137, in fun_wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 76, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 70, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 262, in obj_func\n",
      "    lml, grad = self.log_marginal_likelihood(\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 527, in log_marginal_likelihood\n",
      "    K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 842, in __call__\n",
      "    K1, K1_gradient = self.k1(X, Y, eval_gradient=True)\n",
      "  File \"C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\", line 2169, in __call__\n",
      "    K = np.inner(X, X) + self.sigma_0 ** 2\n",
      "  File \"<__array_function__ internals>\", line 180, in inner\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.82 GiB for an array with shape (15612, 15612) and data type float64\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             R-squared\n",
      "Grad. Boost   0.002416\n",
      "HG Boost      0.002353\n",
      "Poisson      -0.000340\n",
      "Bayesian     -0.000343\n",
      "Ridge        -0.000343\n",
      "OLS          -0.000343\n",
      "Lasso        -0.000344\n",
      "Gamma        -0.000628\n",
      "Inverse      -0.000628\n",
      "LARS         -0.000628\n",
      "NuSupportVM  -0.007027\n",
      "Stacking     -0.025756\n",
      "Voting       -0.114816\n",
      "XGBoost      -0.183408\n",
      "SupportVM    -0.190952\n",
      "AdaBoost DT  -0.294332\n",
      "Linear SVM   -0.318005\n",
      "Dec Forest   -0.340857\n",
      "Extra Trees  -0.497498\n",
      "GaussianP          NaN\n"
     ]
    }
   ],
   "source": [
    "def fit_crossvalidate_reg(df, label, k=10, r=5, repeat=True):\n",
    "  import sklearn.linear_model as lm, pandas as pd, sklearn.ensemble as se\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  from numpy import mean, std\n",
    "  from sklearn import svm\n",
    "  from sklearn import gaussian_process\n",
    "  from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "  from xgboost import XGBRegressor\n",
    "\n",
    "  X = df.drop(columns=[label])\n",
    "  y = df[label]\n",
    "\n",
    "  if repeat:\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=12345)\n",
    "  else:\n",
    "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "  \n",
    "  fit = {}    # Use this to store each of the fit metrics\n",
    "  models = {} # Use this to store each of the models\n",
    "\n",
    "  # Create the model objects\n",
    "  model_ols = lm.LinearRegression()\n",
    "  model_rr = lm.Ridge(alpha=0.5) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "  model_lr = lm.Lasso(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "  model_llr = lm.LassoLars(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "  model_br = lm.BayesianRidge()\n",
    "  model_pr = lm.TweedieRegressor(power=1, link=\"log\") # Power=1 means this is a Poisson\n",
    "  model_gr = lm.TweedieRegressor(power=2, link=\"log\") # Power=2 means this is a Gamma\n",
    "  model_igr = lm.TweedieRegressor(power=3) # Power=3 means this is an inverse Gamma\n",
    "  model_svm = svm.SVR()\n",
    "  model_lsvm = svm.LinearSVR()\n",
    "  model_nusvm = svm.NuSVR()\n",
    "  model_gpr = gaussian_process.GaussianProcessRegressor(DotProduct() + WhiteKernel())\n",
    "  model_df = se.RandomForestRegressor(random_state=12345)\n",
    "  model_etr = se.ExtraTreesRegressor(random_state=12345)\n",
    "  model_abr = se.AdaBoostRegressor(n_estimators=100, random_state=12345)\n",
    "  model_gbr = se.GradientBoostingRegressor(random_state=12345)\n",
    "  model_hgbr = se.HistGradientBoostingRegressor(random_state=12345)\n",
    "  model_vr = se.VotingRegressor(estimators=[('DF', model_df), ('ETR', model_etr), ('ABR', model_abr), ('GBR', model_gbr)])\n",
    "  estimators = [('ridge', lm.RidgeCV()), ('lasso', lm.LassoCV(random_state=42)), ('svr', svm.SVR(C=1, gamma=1e-6))]\n",
    "  model_sr = se.StackingRegressor(estimators=estimators, final_estimator=se.GradientBoostingRegressor(random_state=12345))\n",
    "  model_xgb = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "  \n",
    "  # Fit a crss-validated R squared score and add it to the dict\n",
    "  fit['OLS'] = mean(cross_val_score(model_ols, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Ridge'] = mean(cross_val_score(model_rr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Lasso'] = mean(cross_val_score(model_lr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['LARS'] = mean(cross_val_score(model_llr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Bayesian'] = mean(cross_val_score(model_br, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Poisson'] = mean(cross_val_score(model_pr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Gamma'] = mean(cross_val_score(model_gr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Inverse'] = mean(cross_val_score(model_igr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['SupportVM'] = mean(cross_val_score(model_svm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Linear SVM'] = mean(cross_val_score(model_lsvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['NuSupportVM'] = mean(cross_val_score(model_nusvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['GaussianP'] = mean(cross_val_score(model_gpr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Dec Forest'] = mean(cross_val_score(model_df, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Extra Trees'] = mean(cross_val_score(model_etr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['AdaBoost DT'] = mean(cross_val_score(model_abr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Grad. Boost'] = mean(cross_val_score(model_gbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['HG Boost'] = mean(cross_val_score(model_hgbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Voting'] = mean(cross_val_score(model_vr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Stacking'] = mean(cross_val_score(model_sr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "\n",
    "  # Add the model to another dict; make sure the keys have the same names as the list above\n",
    "  models['OLS'] = model_ols\n",
    "  models['Ridge'] = model_rr\n",
    "  models['Lasso'] = model_lr\n",
    "  models['LARS'] = model_llr\n",
    "  models['Bayesian'] = model_br\n",
    "  models['Poisson'] = model_pr\n",
    "  models['Gamma'] = model_gr\n",
    "  models['Inverse'] = model_igr\n",
    "  models['SupportVM'] = model_svm\n",
    "  models['Linear SVM'] = model_lsvm\n",
    "  models['NuSupportVM'] = model_nusvm\n",
    "  models['GaussianP'] = model_gpr\n",
    "  models['Dec Forest'] = model_df\n",
    "  models['Extra Trees'] = model_etr\n",
    "  models['AdaBoost DT'] = model_abr\n",
    "  models['Grad. Boost'] = model_gbr\n",
    "  models['HG Boost'] = model_hgbr\n",
    "  models['Voting'] = model_vr\n",
    "  models['Stacking'] = model_sr\n",
    "  models['XGBoost'] = model_xgb\n",
    "\n",
    "  # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "  df_fit = pd.DataFrame({'R-squared':fit})\n",
    "  df_fit.sort_values(by=['R-squared'], ascending=False, inplace=True)\n",
    "  best_model = df_fit.index[0]\n",
    "  print(df_fit)\n",
    "\n",
    "  return models[best_model].fit(X, y)\n",
    "\n",
    "# Now, see how this fits into the pipeline:---------------------------------------------\n",
    "# Data cleaning and preparation pipeline\n",
    "df = get_data('CuperCut_TrainCrash.csv', ['CRASH_ID'])\n",
    "df = bin_groups(df)\n",
    "df = drop_columns_missing_50(df)\n",
    "df = impute_mean(df)\n",
    "\n",
    "# Feature selection and modeling pipeline\n",
    "df = fs_variance(df, label=\"CRASH_SEVERITY_ID\", p=.5)\n",
    "model = fit_crossvalidate_reg(df, 'CRASH_SEVERITY_ID', 5, 2)\n",
    "\n",
    "# Deploy/store the model\n",
    "dump_pickle(model, 'best_reg_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSIFICATION ALOGORITHMS\n",
    "# Classification models are those which predict a categorical dependent variable\n",
    "# Let's look at a similar function to fit_crossvalidate_reg() but for classification algorithms\n",
    "# Note some of the differences (e.g. model objects, scoring) and some of the similarities \n",
    "\n",
    "def fit_crossvalidate_clf(df, label, k=10, r=5, repeat=True):\n",
    "  import sklearn.linear_model as lm, pandas as pd, sklearn.ensemble as se, numpy as np\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  from numpy import mean, std\n",
    "  from sklearn import svm\n",
    "  from sklearn import gaussian_process\n",
    "  from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "  from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "  from sklearn import svm\n",
    "  from sklearn.naive_bayes import CategoricalNB\n",
    "  from xgboost import XGBClassifier\n",
    "  from sklearn import preprocessing\n",
    "  from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "  X = df.drop(columns=[label])\n",
    "  y = df[label]\n",
    "\n",
    "  if repeat:\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=12345)\n",
    "  else:\n",
    "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "  \n",
    "  fit = {}    # Use this to store each of the fit metrics\n",
    "  models = {} # Use this to store each of the models\n",
    "  \n",
    "  # Create the model objects\n",
    "  model_log = lm.LogisticRegression(max_iter=100)\n",
    "  model_logcv = lm.RidgeClassifier()\n",
    "  model_sgd = lm.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "  model_pa = lm.PassiveAggressiveClassifier(max_iter=1000, random_state=12345, tol=1e-3)\n",
    "  model_per = lm.Perceptron(fit_intercept=False, max_iter=10, tol=None, shuffle=False)\n",
    "  model_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "  model_svm = svm.SVC(decision_function_shape='ovo') # Remove the parameter for two-class model\n",
    "  model_nb = CategoricalNB()\n",
    "  model_bag = se.BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "  model_ada = se.AdaBoostClassifier(n_estimators=100, random_state=12345)\n",
    "  model_ext = se.ExtraTreesClassifier(n_estimators=100, random_state=12345)\n",
    "  model_rf = se.RandomForestClassifier(n_estimators=10)\n",
    "  model_hgb = se.HistGradientBoostingClassifier(max_iter=100)\n",
    "  model_vot = se.VotingClassifier(estimators=[('lr', model_log), ('rf', model_ext), ('gnb', model_hgb)], voting='hard')\n",
    "  model_gb = se.GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "  estimators = [('ridge', lm.RidgeCV()), ('lasso', lm.LassoCV(random_state=12345)), ('knr', KNeighborsRegressor(n_neighbors=20, metric='euclidean'))]\n",
    "  final_estimator = se.GradientBoostingRegressor(n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1, random_state=12345)\n",
    "  model_st = se.StackingRegressor(estimators=estimators, final_estimator=final_estimator)\n",
    "  model_xgb = XGBClassifier()\n",
    "  model_nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=12345)\n",
    "\n",
    "  # Fit a cross-validated R squared score and add it to the dict\n",
    "  fit['Logistic'] = mean(cross_val_score(model_log, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Ridge'] = mean(cross_val_score(model_logcv, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['SGD'] = mean(cross_val_score(model_sgd, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['PassiveAggressive'] = mean(cross_val_score(model_pa, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Perceptron'] = mean(cross_val_score(model_per, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['KNN'] = mean(cross_val_score(model_knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['SVM'] = mean(cross_val_score(model_svm, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['NaiveBayes'] = mean(cross_val_score(model_nb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Bagging'] = mean(cross_val_score(model_bag, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['AdaBoost'] = mean(cross_val_score(model_ada, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['ExtraTrees'] = mean(cross_val_score(model_ext, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['RandomForest'] = mean(cross_val_score(model_rf, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['HistGradient'] = mean(cross_val_score(model_hgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Voting'] = mean(cross_val_score(model_vot, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['GradBoost'] = mean(cross_val_score(model_gb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['NeuralN'] = mean(cross_val_score(model_nn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "\n",
    "  # Add the model to another dictionary; make sure the keys have the same names as the list above\n",
    "  models['Logistic'] = model_log\n",
    "  models['Ridge'] = model_logcv\n",
    "  models['SGD'] = model_sgd\n",
    "  models['PassiveAggressive'] = model_pa\n",
    "  models['Perceptron'] = model_per\n",
    "  models['KNN'] = model_knn\n",
    "  models['SVM'] = model_svm\n",
    "  models['NaiveBayes'] = model_nb\n",
    "  models['Bagging'] = model_bag\n",
    "  models['AdaBoost'] = model_ada\n",
    "  models['ExtraTrees'] = model_ext\n",
    "  models['RandomForest'] = model_rf\n",
    "  models['HistGradient'] = model_hgb\n",
    "  models['Voting'] = model_vot\n",
    "  models['GradBoost'] = model_gb\n",
    "  models['XGBoost'] = model_xgb\n",
    "  models['NeuralN'] = model_nn\n",
    "\n",
    "  # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "  df_fit = pd.DataFrame({'Accuracy':fit})\n",
    "  df_fit.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
    "  best_model = df_fit.index[0]\n",
    "  print(df_fit)\n",
    "\n",
    "  return models[best_model].fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Accuracy\n",
      "AdaBoost           0.706621\n",
      "HistGradient       0.706621\n",
      "Logistic           0.706621\n",
      "NeuralN            0.706621\n",
      "Ridge              0.706621\n",
      "SVM                0.706621\n",
      "Voting             0.706621\n",
      "GradBoost          0.706416\n",
      "XGBoost            0.703136\n",
      "Bagging            0.691478\n",
      "Perceptron         0.650331\n",
      "KNN                0.647689\n",
      "SGD                0.626195\n",
      "ExtraTrees         0.601824\n",
      "RandomForest       0.593394\n",
      "PassiveAggressive  0.551496\n",
      "NaiveBayes              NaN\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and preparation pipeline\n",
    "df = get_data('CuperCut_TrainCrash.csv', ['CRASH_ID'])\n",
    "df = bin_groups(df)\n",
    "df = drop_columns_missing_50(df)\n",
    "\n",
    "# Drop the label so it does not get dummy-coded, then join it back in after\n",
    "df = impute_mean(df.drop(columns=[\"CRASH_SEVERITY_ID\"])).join(df.CRASH_SEVERITY_ID)\n",
    "\n",
    "# Feature selection and modeling pipeline\n",
    "df = fs_variance(df, label=\"CRASH_SEVERITY_ID\", p=.5)\n",
    "model = fit_crossvalidate_clf(df, \"CRASH_SEVERITY_ID\", 5, 2)\n",
    "\n",
    "# Deployment pipeline\n",
    "dump_pickle(model, 'best_clf_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In both the fit_crossvalidate_reg() and fit_crossvalidate_clf() functions, we are testing\n",
    "# all of the algorithms with a specific set of parameters..\n",
    "# (note: this concept can be applied to any algorithm)\n",
    "\n",
    "df = get_data('CuperCut_TrainCrash.csv', ['CRASH_ID'])\n",
    "df = bin_groups(df)\n",
    "df = drop_columns_missing_50(df)\n",
    "df = impute_mean(df)\n",
    "df = fs_variance(df, label=\"CRASH_SEVERITY_ID\", p=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=3, random_state=None, shuffle=False),\n",
       "             estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                    colsample_bylevel=None,\n",
       "                                    colsample_bynode=None,\n",
       "                                    colsample_bytree=None,\n",
       "                                    enable_categorical=False, gamma=None,\n",
       "                                    gpu_id=None, importance_type=None,\n",
       "                                    interaction_constraints=None,\n",
       "                                    learning_rate=None, max_delta_step=None,\n",
       "                                    max_depth=None, min_child_wei...\n",
       "                                    n_estimators=100, n_jobs=None,\n",
       "                                    num_parallel_tree=None, predictor=None,\n",
       "                                    random_state=None, reg_alpha=None,\n",
       "                                    reg_lambda=None, scale_pos_weight=None,\n",
       "                                    subsample=None, tree_method=None,\n",
       "                                    validate_parameters=None, verbosity=None),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'booster': ['gbtree', 'gblinear', 'dart'],\n",
       "                         'learning_rate': [0.1, 0.3, 0.5],\n",
       "                         'objective': ['reg:squarederror']},\n",
       "             scoring='r2', verbose=2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"booster\": ['gbtree', 'gblinear', 'dart'],\n",
    "    \"learning_rate\": [0.1, 0.3, 0.5], \n",
    "    \"objective\": ['reg:squarederror'], \n",
    "}\n",
    "\n",
    "# Create the hypertuning search object\n",
    "model_xgb = GridSearchCV(\n",
    "    XGBRegressor(), \n",
    "    params, \n",
    "    n_jobs=-1, # Number of threads to use; -1 means use all available\n",
    "    scoring='r2', # Options: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    cv=KFold(n_splits=3), # Choose any type of cross_validation you want\n",
    "    verbose=2, # How much information to display in the results; options: 1, 2, or 3\n",
    "    refit=True # This saves the best-fitting model\n",
    "    )\n",
    "\n",
    "model_xgb.fit(df.drop(columns=['CRASH_SEVERITY_ID']), df.CRASH_SEVERITY_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'booster': 'gblinear', 'learning_rate': 0.1, 'objective': 'reg:squarederror'}\n",
      "R-squared:\t -0.0050218722741327175\n",
      "All results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_booster</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_objective</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.033247</td>\n",
       "      <td>0.076533</td>\n",
       "      <td>0.009520</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.1</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>{'booster': 'gbtree', 'learning_rate': 0.1, 'o...</td>\n",
       "      <td>-0.004882</td>\n",
       "      <td>-0.017937</td>\n",
       "      <td>-0.007432</td>\n",
       "      <td>-0.010084</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.028746</td>\n",
       "      <td>0.048612</td>\n",
       "      <td>0.011256</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.3</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>{'booster': 'gbtree', 'learning_rate': 0.3, 'o...</td>\n",
       "      <td>-0.038542</td>\n",
       "      <td>-0.047962</td>\n",
       "      <td>-0.042925</td>\n",
       "      <td>-0.043143</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.048402</td>\n",
       "      <td>0.123369</td>\n",
       "      <td>0.009998</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.5</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>{'booster': 'gbtree', 'learning_rate': 0.5, 'o...</td>\n",
       "      <td>-0.082128</td>\n",
       "      <td>-0.065993</td>\n",
       "      <td>-0.088139</td>\n",
       "      <td>-0.078753</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.272891</td>\n",
       "      <td>0.024977</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>0.1</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>{'booster': 'gblinear', 'learning_rate': 0.1, ...</td>\n",
       "      <td>-0.004119</td>\n",
       "      <td>-0.009710</td>\n",
       "      <td>-0.001237</td>\n",
       "      <td>-0.005022</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.224862</td>\n",
       "      <td>0.018369</td>\n",
       "      <td>0.005913</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>0.3</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>{'booster': 'gblinear', 'learning_rate': 0.3, ...</td>\n",
       "      <td>-0.004357</td>\n",
       "      <td>-0.009366</td>\n",
       "      <td>-0.001367</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.202708</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>0.5</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>{'booster': 'gblinear', 'learning_rate': 0.5, ...</td>\n",
       "      <td>-0.004357</td>\n",
       "      <td>-0.009366</td>\n",
       "      <td>-0.001367</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.287942</td>\n",
       "      <td>0.019862</td>\n",
       "      <td>0.103524</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>dart</td>\n",
       "      <td>0.1</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>{'booster': 'dart', 'learning_rate': 0.1, 'obj...</td>\n",
       "      <td>-0.004882</td>\n",
       "      <td>-0.017937</td>\n",
       "      <td>-0.007432</td>\n",
       "      <td>-0.010084</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.323068</td>\n",
       "      <td>0.013203</td>\n",
       "      <td>0.071453</td>\n",
       "      <td>0.011080</td>\n",
       "      <td>dart</td>\n",
       "      <td>0.3</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>{'booster': 'dart', 'learning_rate': 0.3, 'obj...</td>\n",
       "      <td>-0.038542</td>\n",
       "      <td>-0.047962</td>\n",
       "      <td>-0.042925</td>\n",
       "      <td>-0.043143</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.073420</td>\n",
       "      <td>1.457672</td>\n",
       "      <td>0.045416</td>\n",
       "      <td>0.011562</td>\n",
       "      <td>dart</td>\n",
       "      <td>0.5</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>{'booster': 'dart', 'learning_rate': 0.5, 'obj...</td>\n",
       "      <td>-0.082128</td>\n",
       "      <td>-0.065993</td>\n",
       "      <td>-0.088139</td>\n",
       "      <td>-0.078753</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_booster  \\\n",
       "0       1.033247      0.076533         0.009520        0.001173        gbtree   \n",
       "1       1.028746      0.048612         0.011256        0.002159        gbtree   \n",
       "2       1.048402      0.123369         0.009998        0.004405        gbtree   \n",
       "3       0.272891      0.024977         0.005368        0.000483      gblinear   \n",
       "4       0.224862      0.018369         0.005913        0.000852      gblinear   \n",
       "5       0.202708      0.002308         0.005506        0.000357      gblinear   \n",
       "6       4.287942      0.019862         0.103524        0.002509          dart   \n",
       "7       4.323068      0.013203         0.071453        0.011080          dart   \n",
       "8       3.073420      1.457672         0.045416        0.011562          dart   \n",
       "\n",
       "  param_learning_rate   param_objective  \\\n",
       "0                 0.1  reg:squarederror   \n",
       "1                 0.3  reg:squarederror   \n",
       "2                 0.5  reg:squarederror   \n",
       "3                 0.1  reg:squarederror   \n",
       "4                 0.3  reg:squarederror   \n",
       "5                 0.5  reg:squarederror   \n",
       "6                 0.1  reg:squarederror   \n",
       "7                 0.3  reg:squarederror   \n",
       "8                 0.5  reg:squarederror   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'booster': 'gbtree', 'learning_rate': 0.1, 'o...          -0.004882   \n",
       "1  {'booster': 'gbtree', 'learning_rate': 0.3, 'o...          -0.038542   \n",
       "2  {'booster': 'gbtree', 'learning_rate': 0.5, 'o...          -0.082128   \n",
       "3  {'booster': 'gblinear', 'learning_rate': 0.1, ...          -0.004119   \n",
       "4  {'booster': 'gblinear', 'learning_rate': 0.3, ...          -0.004357   \n",
       "5  {'booster': 'gblinear', 'learning_rate': 0.5, ...          -0.004357   \n",
       "6  {'booster': 'dart', 'learning_rate': 0.1, 'obj...          -0.004882   \n",
       "7  {'booster': 'dart', 'learning_rate': 0.3, 'obj...          -0.038542   \n",
       "8  {'booster': 'dart', 'learning_rate': 0.5, 'obj...          -0.082128   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0          -0.017937          -0.007432        -0.010084        0.005650   \n",
       "1          -0.047962          -0.042925        -0.043143        0.003849   \n",
       "2          -0.065993          -0.088139        -0.078753        0.009350   \n",
       "3          -0.009710          -0.001237        -0.005022        0.003518   \n",
       "4          -0.009366          -0.001367        -0.005030        0.003300   \n",
       "5          -0.009366          -0.001367        -0.005030        0.003300   \n",
       "6          -0.017937          -0.007432        -0.010084        0.005650   \n",
       "7          -0.047962          -0.042925        -0.043143        0.003849   \n",
       "8          -0.065993          -0.088139        -0.078753        0.009350   \n",
       "\n",
       "   rank_test_score  \n",
       "0                5  \n",
       "1                6  \n",
       "2                9  \n",
       "3                1  \n",
       "4                2  \n",
       "5                3  \n",
       "6                4  \n",
       "7                7  \n",
       "8                8  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Best parameters: {model_xgb.best_params_}')\n",
    "print(f'R-squared:\\t {model_xgb.best_score_}')\n",
    "\n",
    "print(f'All results:')\n",
    "pd.DataFrame(model_xgb.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 100\n",
      "max_resources_: 800\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 32\n",
      "n_resources: 100\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 16\n",
      "n_resources: 200\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 8\n",
      "n_resources: 400\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 4\n",
      "n_resources: 800\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HalvingRandomSearchCV(cv=KFold(n_splits=3, random_state=None, shuffle=False),\n",
       "                      estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                             colsample_bylevel=None,\n",
       "                                             colsample_bynode=None,\n",
       "                                             colsample_bytree=None,\n",
       "                                             enable_categorical=False,\n",
       "                                             gamma=None, gpu_id=None,\n",
       "                                             importance_type=None,\n",
       "                                             interaction_constraints=None,\n",
       "                                             learning_rate=None,\n",
       "                                             max_delta_step=None,\n",
       "                                             max_depth=None,\n",
       "                                             min_...\n",
       "                      factor=2, max_resources=800, min_resources=100,\n",
       "                      n_candidates=32, n_jobs=-1,\n",
       "                      param_distributions={'alpha': [0, 0.001, 1, 100],\n",
       "                                           'booster': ['gbtree', 'gblinear',\n",
       "                                                       'dart'],\n",
       "                                           'colsample_bytree': [0.8, 1],\n",
       "                                           'gamma': [0, 0.1, 0.2],\n",
       "                                           'learning_rate': [0.1, 0.3, 0.5],\n",
       "                                           'max_depth': [3, 6, 9],\n",
       "                                           'min_child_weight': [1, 2, 3],\n",
       "                                           'objective': ['reg:squarederror'],\n",
       "                                           'subsample': [0.8, 0.9, 1]},\n",
       "                      resource='n_estimators', scoring='r2', verbose=2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Halving is the process of training a selection of models on very small datasets to improve speed. \n",
    "# Then, much like a tournament, the best n models are selected and trained again using a slightly larger dataset. \n",
    "# The advantage over RandomizedSearchCV is that every model gets trained at least once. \n",
    "# The potential downside is that a very good hyperparameter set may simply happen to get a poor sample to \n",
    "# train with and end up being missed.\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv # Must import this first\n",
    "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "\n",
    "params = {\n",
    "    \"booster\": ['gbtree', 'gblinear', 'dart'], # Default is gbtree\n",
    "    \"learning_rate\": [0.1, 0.3, 0.5],  # It accepts float [0,1] specifying learning rate for training process. Default = 0.3\n",
    "    \"objective\": ['reg:squarederror'], # List of possible values: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "    \"max_depth\": [3, 6, 9], # Must be between 3-10; default = 6\n",
    "    \"min_child_weight\": [1, 2, 3], # Default = 1\n",
    "    \"gamma\": [0, 0.1, 0.2], # Default = 0\n",
    "    \"subsample\": [0.8, 0.9, 1], # Default = 1\n",
    "    \"colsample_bytree\": [0.8, 1], # Default = 1\n",
    "    \"alpha\": [0, .001, 1, 100], # Default = 0\n",
    "}\n",
    "\n",
    "# Create the hypertuning object\n",
    "model_xgb = HalvingRandomSearchCV( # If this takes to long, change it to HalvingRandomSearchCV\n",
    "    XGBRegressor(), \n",
    "    params,\n",
    "    factor=2, # The 'halving' parameter; proportion of candidates selected for each iteration\n",
    "    n_candidates=32, # The number of hyperparameter value sets to randomly sample\n",
    "    resource='n_estimators', # Default = n_samples, but use n_estimators for boosting algorithms\n",
    "    n_jobs=-1, # Number of threads to use; -1 means use all available\n",
    "    scoring='r2', # Options: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    cv=KFold(n_splits=3), # Choose any type of cross_validation you want\n",
    "    verbose=2, # How much information to display in the results; options: 1, 2, or 3\n",
    "    max_resources=800, # The maximum number of resources (either n_samples or n_estimators) to use in each round\n",
    "    min_resources=100, # The maximum number of resources (either n_samples or n_estimators) to use in each round\n",
    "    refit=True # This saves the best-fitting model\n",
    "    )\n",
    "\n",
    "model_xgb.fit(df.drop(columns=['CRASH_SEVERITY_ID']), df.CRASH_SEVERITY_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'subsample': 1, 'objective': 'reg:squarederror', 'min_child_weight': 2, 'max_depth': 6, 'learning_rate': 0.5, 'gamma': 0.2, 'colsample_bytree': 0.8, 'booster': 'dart', 'alpha': 100, 'n_estimators': 800}\n",
      "R-squared:\t -0.001773257616376922\n",
      "All results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Mean Fit Score</th>\n",
       "      <th>Std Fit Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.001965</td>\n",
       "      <td>0.005567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.001773</td>\n",
       "      <td>0.005798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005167</td>\n",
       "      <td>0.003485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.098468</td>\n",
       "      <td>0.008246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>0.003365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.020612</td>\n",
       "      <td>0.005194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.010118</td>\n",
       "      <td>0.006795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>0.003365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.048449</td>\n",
       "      <td>0.005969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.002190</td>\n",
       "      <td>0.005042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.020714</td>\n",
       "      <td>0.007825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.011643</td>\n",
       "      <td>0.005473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.027683</td>\n",
       "      <td>0.005049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005361</td>\n",
       "      <td>0.003355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>0.005618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.031458</td>\n",
       "      <td>0.005719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.002306</td>\n",
       "      <td>0.005573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.031290</td>\n",
       "      <td>0.006138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.036393</td>\n",
       "      <td>0.008216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.049114</td>\n",
       "      <td>0.020263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.214718</td>\n",
       "      <td>0.020195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>0.003365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.071148</td>\n",
       "      <td>0.001209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.089759</td>\n",
       "      <td>0.007306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.010540</td>\n",
       "      <td>0.005930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005167</td>\n",
       "      <td>0.003485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>0.003366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>0.003366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>0.003365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>0.003365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>0.003365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.004824</td>\n",
       "      <td>0.006898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.002297</td>\n",
       "      <td>0.004850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.001996</td>\n",
       "      <td>0.005334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.002024</td>\n",
       "      <td>0.005338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.001773</td>\n",
       "      <td>0.005798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005030</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.013515</td>\n",
       "      <td>0.005754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.002091</td>\n",
       "      <td>0.005137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.001974</td>\n",
       "      <td>0.005177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.001975</td>\n",
       "      <td>0.005181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.001773</td>\n",
       "      <td>0.005798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.002064</td>\n",
       "      <td>0.005026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.001981</td>\n",
       "      <td>0.005068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.001928</td>\n",
       "      <td>0.005071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.001773</td>\n",
       "      <td>0.005798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Parameters  Mean Fit Score  \\\n",
       "0   {'subsample': 1, 'objective': 'reg:squarederro...       -0.005030   \n",
       "1   {'subsample': 0.9, 'objective': 'reg:squareder...       -0.001965   \n",
       "2   {'subsample': 1, 'objective': 'reg:squarederro...       -0.001773   \n",
       "3   {'subsample': 0.9, 'objective': 'reg:squareder...       -0.005167   \n",
       "4   {'subsample': 1, 'objective': 'reg:squarederro...       -0.005030   \n",
       "5   {'subsample': 1, 'objective': 'reg:squarederro...       -0.098468   \n",
       "6   {'subsample': 0.9, 'objective': 'reg:squareder...       -0.005149   \n",
       "7   {'subsample': 1, 'objective': 'reg:squarederro...       -0.020612   \n",
       "8   {'subsample': 1, 'objective': 'reg:squarederro...       -0.010118   \n",
       "9   {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005149   \n",
       "10  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005030   \n",
       "11  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005030   \n",
       "12  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.048449   \n",
       "13  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.002190   \n",
       "14  {'subsample': 1, 'objective': 'reg:squarederro...       -0.005030   \n",
       "15  {'subsample': 1, 'objective': 'reg:squarederro...       -0.020714   \n",
       "16  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.011643   \n",
       "17  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.027683   \n",
       "18  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005361   \n",
       "19  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.001969   \n",
       "20  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.031458   \n",
       "21  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.002306   \n",
       "22  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.031290   \n",
       "23  {'subsample': 1, 'objective': 'reg:squarederro...       -0.036393   \n",
       "24  {'subsample': 1, 'objective': 'reg:squarederro...       -0.049114   \n",
       "25  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005030   \n",
       "26  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.214718   \n",
       "27  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005149   \n",
       "28  {'subsample': 1, 'objective': 'reg:squarederro...       -0.071148   \n",
       "29  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.089759   \n",
       "30  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.010540   \n",
       "31  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.005167   \n",
       "32  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.005149   \n",
       "33  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.005149   \n",
       "34  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005149   \n",
       "35  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005149   \n",
       "36  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.005149   \n",
       "37  {'subsample': 1, 'objective': 'reg:squarederro...       -0.005030   \n",
       "38  {'subsample': 1, 'objective': 'reg:squarederro...       -0.005030   \n",
       "39  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005030   \n",
       "40  {'subsample': 1, 'objective': 'reg:squarederro...       -0.005030   \n",
       "41  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005030   \n",
       "42  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005030   \n",
       "43  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.004824   \n",
       "44  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.002297   \n",
       "45  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.001996   \n",
       "46  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.002024   \n",
       "47  {'subsample': 1, 'objective': 'reg:squarederro...       -0.001773   \n",
       "48  {'subsample': 1, 'objective': 'reg:squarederro...       -0.005030   \n",
       "49  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005030   \n",
       "50  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005030   \n",
       "51  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.013515   \n",
       "52  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.002091   \n",
       "53  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.001974   \n",
       "54  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.001975   \n",
       "55  {'subsample': 1, 'objective': 'reg:squarederro...       -0.001773   \n",
       "56  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.002064   \n",
       "57  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.001981   \n",
       "58  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.001928   \n",
       "59  {'subsample': 1, 'objective': 'reg:squarederro...       -0.001773   \n",
       "\n",
       "    Std Fit Score  \n",
       "0        0.003300  \n",
       "1        0.005567  \n",
       "2        0.005798  \n",
       "3        0.003485  \n",
       "4        0.003300  \n",
       "5        0.008246  \n",
       "6        0.003365  \n",
       "7        0.005194  \n",
       "8        0.006795  \n",
       "9        0.003365  \n",
       "10       0.003300  \n",
       "11       0.003300  \n",
       "12       0.005969  \n",
       "13       0.005042  \n",
       "14       0.003300  \n",
       "15       0.007825  \n",
       "16       0.005473  \n",
       "17       0.005049  \n",
       "18       0.003355  \n",
       "19       0.005618  \n",
       "20       0.005719  \n",
       "21       0.005573  \n",
       "22       0.006138  \n",
       "23       0.008216  \n",
       "24       0.020263  \n",
       "25       0.003300  \n",
       "26       0.020195  \n",
       "27       0.003365  \n",
       "28       0.001209  \n",
       "29       0.007306  \n",
       "30       0.005930  \n",
       "31       0.003485  \n",
       "32       0.003366  \n",
       "33       0.003366  \n",
       "34       0.003365  \n",
       "35       0.003365  \n",
       "36       0.003365  \n",
       "37       0.003300  \n",
       "38       0.003300  \n",
       "39       0.003300  \n",
       "40       0.003300  \n",
       "41       0.003300  \n",
       "42       0.003300  \n",
       "43       0.006898  \n",
       "44       0.004850  \n",
       "45       0.005334  \n",
       "46       0.005338  \n",
       "47       0.005798  \n",
       "48       0.003300  \n",
       "49       0.003300  \n",
       "50       0.003300  \n",
       "51       0.005754  \n",
       "52       0.005137  \n",
       "53       0.005177  \n",
       "54       0.005181  \n",
       "55       0.005798  \n",
       "56       0.005026  \n",
       "57       0.005068  \n",
       "58       0.005071  \n",
       "59       0.005798  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Best parameters: {model_xgb.best_params_}')\n",
    "print(f'R-squared:\\t {model_xgb.best_score_}')\n",
    "\n",
    "print(f'All results:')\n",
    "pd.DataFrame({\n",
    "    \"Parameters\":model_xgb.cv_results_['params'], \n",
    "    \"Mean Fit Score\":model_xgb.cv_results_['mean_test_score'],\n",
    "    \"Std Fit Score\":model_xgb.cv_results_['std_test_score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'subsample': 0.8, 'objective': 'reg:squarederror', 'min_child_weight': 3, 'max_depth': 9, 'learning_rate': 0.1, 'gamma': 0.2, 'colsample_bytree': 0.8, 'booster': 'gbtree', 'alpha': 100}\n",
      "R-squared:\t -0.0017696554330600318\n",
      "All results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Mean Fit Score</th>\n",
       "      <th>Std Fit Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.005361</td>\n",
       "      <td>0.003355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.001770</td>\n",
       "      <td>0.005383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.005361</td>\n",
       "      <td>0.003355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.005022</td>\n",
       "      <td>0.003518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.042550</td>\n",
       "      <td>0.003654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.001965</td>\n",
       "      <td>0.005567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.002392</td>\n",
       "      <td>0.004693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'subsample': 1, 'objective': 'reg:squarederro...</td>\n",
       "      <td>-0.016942</td>\n",
       "      <td>0.012636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'subsample': 0.9, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.002170</td>\n",
       "      <td>0.005267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'subsample': 0.8, 'objective': 'reg:squareder...</td>\n",
       "      <td>-0.057439</td>\n",
       "      <td>0.006133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Parameters  Mean Fit Score  \\\n",
       "0  {'subsample': 1, 'objective': 'reg:squarederro...       -0.005361   \n",
       "1  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.001770   \n",
       "2  {'subsample': 1, 'objective': 'reg:squarederro...       -0.005361   \n",
       "3  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.005022   \n",
       "4  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.042550   \n",
       "5  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.001965   \n",
       "6  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.002392   \n",
       "7  {'subsample': 1, 'objective': 'reg:squarederro...       -0.016942   \n",
       "8  {'subsample': 0.9, 'objective': 'reg:squareder...       -0.002170   \n",
       "9  {'subsample': 0.8, 'objective': 'reg:squareder...       -0.057439   \n",
       "\n",
       "   Std Fit Score  \n",
       "0       0.003355  \n",
       "1       0.005383  \n",
       "2       0.003355  \n",
       "3       0.003518  \n",
       "4       0.003654  \n",
       "5       0.005567  \n",
       "6       0.004693  \n",
       "7       0.012636  \n",
       "8       0.005267  \n",
       "9       0.006133  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomized Parameter Optimization allows us to randomly select n parameter combinations\n",
    "# What are some pros/cons of this approach?\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "    \"booster\": ['gbtree', 'gblinear', 'dart'], # Default is gbtree\n",
    "    \"learning_rate\": [0.1, 0.3, 0.5],  # It accepts float [0,1] specifying learning rate for training process. Default = 0.3\n",
    "    \"objective\": ['reg:squarederror'], # List of possible values: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "    \"max_depth\": [3, 6, 9], # Must be between 3-10; default = 6\n",
    "    \"min_child_weight\": [1, 2, 3], # Default = 1\n",
    "    \"gamma\": [0, 0.1, 0.2], # Default = 0\n",
    "    \"subsample\": [0.8, 0.9, 1], # Default = 1\n",
    "    \"colsample_bytree\": [0.8, 1], # Default = 1\n",
    "    \"alpha\": [0, .001, 1, 100], # Default = 0\n",
    "}\n",
    "\n",
    "# Create the hypertuning object\n",
    "model_xgb = RandomizedSearchCV(\n",
    "    XGBRegressor(), \n",
    "    params,\n",
    "    n_iter=10, # Number of random samples to fit; default is 10\n",
    "    n_jobs=-1, # Number of threads to use; -1 means use all available\n",
    "    scoring='r2', # Options: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    cv=KFold(n_splits=3), # Choose any type of cross_validation you want\n",
    "    verbose=2, # How much information to display in the results; options: 1, 2, or 3\n",
    "    refit=True # This saves the best-fitting model\n",
    "    )\n",
    "\n",
    "model_xgb.fit(df.drop(columns=['CRASH_SEVERITY_ID']), df.CRASH_SEVERITY_ID)\n",
    "\n",
    "print(f'Best parameters: {model_xgb.best_params_}')\n",
    "print(f'R-squared:\\t {model_xgb.best_score_}')\n",
    "\n",
    "print(f'All results:')\n",
    "pd.DataFrame({\n",
    "    \"Parameters\":model_xgb.cv_results_['params'], \n",
    "    \"Mean Fit Score\":model_xgb.cv_results_['mean_test_score'],\n",
    "    \"Std Fit Score\":model_xgb.cv_results_['std_test_score']\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ff7ad65fd33a767da5485be53880310ae0b5bdc0d199936e60539568ae78205"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
