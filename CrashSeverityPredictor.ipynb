{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place for all the Functions\n",
    "\n",
    "#This is grabbing data\n",
    "def get_data(url, drop=[]): \n",
    "  import pandas as pd\n",
    "  df = pd.read_csv(url)\n",
    "  if len(drop) > 0:\n",
    "    for col in drop:\n",
    "      df.drop(columns=[col], inplace=True)\n",
    "  return df\n",
    "\n",
    "#Anything that did not appear more than 5% of the time it gets put into this group\n",
    "def bin_groups(df, percent=.05):\n",
    "  import pandas as pd\n",
    "  for col in df:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "      for group, count in df[col].value_counts().iteritems():\n",
    "        if count / len(df) < percent:\n",
    "          df.loc[df[col] == group, col] = 'Other'\n",
    "  return df\n",
    "\n",
    "#Drop columns that have more than 50% missing\n",
    "def drop_columns_missing_50(df, cutoff=.5):\n",
    "  import pandas as pd\n",
    "  for col in df:\n",
    "    if df[col].isna().sum() / len(df) > cutoff:\n",
    "      df.drop(columns=[col], inplace=True)\n",
    "  return df\n",
    "\n",
    "#This does something haha\n",
    "def impute_mean(df):\n",
    "  from sklearn.impute import SimpleImputer\n",
    "  import pandas as pd, numpy as np\n",
    "  for col in df:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "  imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "  return df\n",
    "\n",
    "#If there is not enough variance, then we will toss it out\n",
    "#This is determined by the \"p\" parameter \n",
    "#This does not take into account Multi Collinearity\n",
    "#But we might want to run this function becuase it is fast\n",
    "def fs_variance(df, label=\"\", p=0.8):\n",
    "  from sklearn.feature_selection import VarianceThreshold\n",
    "  import pandas as pd\n",
    "\n",
    "  if label != \"\":\n",
    "    X = df.drop(columns=[label])\n",
    "    \n",
    "  sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
    "  sel.fit_transform(X)\n",
    "\n",
    "  # Add the label back in after removing poor features\n",
    "  return df[sel.get_feature_names_out()].join(df[label])\n",
    "\n",
    "#The \"k\" represents how many splits we want\n",
    "#We are splitting out data frame into our features and our label\n",
    "#Setting up our cross validation object\n",
    "#say \"k\" is 5, that means we are splitting our data into 5 train/test sets\n",
    "#This then means we will get 5 different values for R2\n",
    "#We will then take the average of these R2 and that will be our Avg R2\n",
    "#This is the R2 we will use to check against\n",
    "def fit_crossvalidate_mlr(df, k, label, repeat=True):\n",
    "  from sklearn.linear_model import LinearRegression\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  import pandas as pd\n",
    "  from numpy import mean, std\n",
    "  X = df.drop(label,axis=1)\n",
    "  y = df[label]\n",
    "  if repeat:\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=5, random_state=12345)\n",
    "  else:\n",
    "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "  scores = cross_val_score(LinearRegression(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "  print(f'Average R-squared:\\t{mean(scores)}')\n",
    "  return LinearRegression().fit(X, y)\n",
    "\n",
    "def dump_pickle(model, file_name):\n",
    "  import pickle\n",
    "  pickle.dump(model, open(file_name, \"wb\"))\n",
    "\n",
    "def load_pickle(file_name):\n",
    "  import pickle\n",
    "  model = pickle.load(open(file_name, \"rb\"))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test set\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = get_data('CuperCut_TrainCrash.csv', [\"CRASH_ID\"])\n",
    "df = bin_groups(df, 0.05)\n",
    "df = drop_columns_missing_50(df)\n",
    "df = impute_mean(df)\n",
    "\n",
    "#df = fs_variance(df, label=\"CRASH_SEVERITY_ID\", p=.5)\n",
    "\n",
    "model = fit_crossvalidate_mlr(df, 5, label='CRASH_SEVERITY_ID')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to standardize our features so they are all on the same scale\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Standardize the data to compare feature importances (standardize data (so we can compare))\n",
    "df_minmax = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Run our fit_crossvalidate_mlr function to get back a trained model\n",
    "model = fit_crossvalidate_mlr(df_minmax, 10, label='CRASH_SEVERITY_ID')\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "  model.coef_,\n",
    "  columns=['Coefficients'], index=df_minmax.drop(columns=['CRASH_SEVERITY_ID']).columns\n",
    ")\n",
    "\n",
    "coefs.sort_values(by=['Coefficients'], ascending=False, inplace=True)\n",
    "\n",
    "coefs.plot(kind='barh', figsize=(10, 30))\n",
    "plt.title('MLR model')\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.subplots_adjust(left=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF TIME\n",
    "\n",
    "def vif(df, label=\"\"):\n",
    "  import pandas as pd\n",
    "  from sklearn.linear_model import LinearRegression\n",
    "  \n",
    "  # initialize dictionaries\n",
    "  vif_dict, tolerance_dict = {}, {}\n",
    "\n",
    "  # drop unnecessary columns if they are found in the dataframe\n",
    "  if label in df.columns: df.drop(columns=[label], inplace=True)\n",
    "  if 'const' in df.columns: df.drop(columns=['const'], inplace=True)\n",
    "\n",
    "  # form input data for each exogenous variable\n",
    "  for col in df:\n",
    "    y = df[col]\n",
    "    X = df.drop(columns=[col])\n",
    "    \n",
    "    # extract r-squared from the fit\n",
    "    r_squared = LinearRegression().fit(X, y).score(X, y)\n",
    "\n",
    "    # calculate VIF\n",
    "    if r_squared < 1: # Prevent division by zero runtime error\n",
    "      vif = 1/(1 - r_squared) \n",
    "    else:\n",
    "      vif = 100\n",
    "    vif_dict[col] = vif\n",
    "\n",
    "    # calculate tolerance\n",
    "    tolerance = 1 - r_squared\n",
    "    tolerance_dict[col] = tolerance\n",
    "\n",
    "  # generate the DataFrame to return\n",
    "  return pd.DataFrame({'VIF': vif_dict, 'Tolerance': tolerance_dict}).sort_values(by=['VIF'], ascending=False)\n",
    "\n",
    "vif(df).head(20)\n",
    "\n",
    "#There doesn't seem to be any complications with Multicoliniarity with the VIF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate feature importance for tree model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "from numpy import mean\n",
    "\n",
    "y = df.CRASH_SEVERITY_ID\n",
    "X = df.drop(columns=['CRASH_SEVERITY_ID'])\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=12345)\n",
    "scores = cross_val_score(DecisionTreeRegressor(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "print(f'Decision Tree R-squared:\\t{mean(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate feature importance for tree model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "from numpy import mean\n",
    "\n",
    "y = df.CRASH_SEVERITY_ID\n",
    "X = df.drop(columns=['CRASH_SEVERITY_ID'])\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=12345)\n",
    "scores = cross_val_score(DecisionTreeRegressor(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "print(f'Decision Tree R-squared:\\t{mean(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "  \n",
    "# Import data and follow prior cleaning steps\n",
    "# (Normal Pipeline)\n",
    "dfv = get_data('CuperCut_TrainCrash.csv', ['CRASH_ID'])\n",
    "dfv = bin_groups(dfv)\n",
    "dfv = drop_columns_missing_50(dfv)\n",
    "dfv = impute_mean(dfv)\n",
    "  \n",
    "# Select features only\n",
    "X = dfv.drop(columns=['CRASH_SEVERITY_ID'])\n",
    "  \n",
    "# View the total number of features before reducing\n",
    "print(X.shape)\n",
    "  \n",
    "# Calculate variance based on desired p-value so that Variance = p(1 - p)\n",
    "# The code below uses p = 0.8 as the cutoff value:\n",
    "p = 0.8\n",
    "sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
    "sel.fit_transform(X)\n",
    "X_reduced = pd.DataFrame(sel.fit_transform(X))\n",
    "  \n",
    "# View the remaining number of features\n",
    "print(X_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Function for the Variance threshold in our pipeline\n",
    "\n",
    "vardf = get_data('CuperCut_TrainCrash.csv', ['CRASH_ID'])\n",
    "vardf = bin_groups(vardf)\n",
    "vardf = drop_columns_missing_50(vardf)\n",
    "vardf = impute_mean(vardf)\n",
    "vardf = fs_variance(vardf, label=\"CRASH_SEVERITY_ID\")\n",
    "model = fit_crossvalidate_mlr(vardf, 10, \"CRASH_SEVERITY_ID\")\n",
    "\n",
    "\n",
    "#MLR With all Features R2 = 0.18966380192691812\n",
    "#MLR with Variance Threshold used R2 = 0.014341587255520503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slect K best feature Selection\n",
    "# the \"k\" represents the number of columns we want to keep\n",
    "def fs_kbest(df, k=10, label=\"\"):\n",
    "  from sklearn.feature_selection import SelectKBest\n",
    "  from sklearn.feature_selection import r_regression\n",
    "  import pandas as pd\n",
    "  \n",
    "  X = df.drop(columns=[label])\n",
    "  y = df[label]\n",
    "\n",
    "  # Select the top k features based on a given bivariate metric\n",
    "  # Looking at each feature and its correlation to the label\n",
    "  # But still not getting rid of the multi colliniarity issue\n",
    "  sel = SelectKBest(r_regression, k=k)\n",
    "  sel.fit_transform(X, y)\n",
    "  \n",
    "  return df[sel.get_feature_names_out()].join(df[label])\n",
    "\n",
    "# Integrate this new function into the pipeline:\n",
    "kdf = get_data('CuperCut_TrainCrash.csv', ['CRASH_ID'])\n",
    "kdf = bin_groups(kdf)\n",
    "kdf = drop_columns_missing_50(kdf)\n",
    "kdf = impute_mean(kdf)\n",
    "kdf = fs_kbest(kdf, 20, label=\"CRASH_SEVERITY_ID\") # Keep top 20 to validly compare to VarianceThreshold example\n",
    "model = fit_crossvalidate_mlr(kdf, 10, \"CRASH_SEVERITY_ID\")\n",
    "\n",
    "#MLR With all Features R2 = 0.18966380192691812\n",
    "#MLR with Variance Threshold used R2 = 0.014341587255520503\n",
    "#MLR with Select K Best with top 9 R2 = 0.17866264279980612\n",
    "#MLR with Select K Best with top 20 R2 = 0.18685250796314865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1 Based Feature Selection:\n",
    "# The first method is based on linear modeling, known as L1-based feature selection (DOES account for Multi Colliniarity BUT is slower)\n",
    "# This take a while so let's Wordle while this runs\n",
    "\n",
    "def fs_select_linear(df, label=\"\"):\n",
    "  from sklearn.svm import LinearSVC\n",
    "  from sklearn.feature_selection import SelectFromModel\n",
    "  import pandas as pd\n",
    "\n",
    "  X = df.drop(label,axis=1)\n",
    "  y = df[label]\n",
    "\n",
    "  # As C increases, more features are kept\n",
    "  lsvc = LinearSVC(C=0.05, penalty=\"l1\", dual=False).fit(X, y)\n",
    "  sel = SelectFromModel(lsvc, prefit=True)\n",
    "  sel.transform(X)\n",
    "\n",
    "  columns = list(X.columns[sel.get_support()])\n",
    "  columns.append(label)\n",
    "  return df[columns]\n",
    "\n",
    "# Integrate this new function into the pipeline:\n",
    "ldf = get_data('CuperCut_TrainCrash.csv', ['CRASH_ID'])\n",
    "ldf = bin_groups(ldf)\n",
    "ldf = drop_columns_missing_50(ldf)\n",
    "ldf = impute_mean(ldf)\n",
    "ldf = fs_select_linear(ldf, label=\"CRASH_SEVERITY_ID\")\n",
    "model = fit_crossvalidate_mlr(ldf, 10, \"CRASH_SEVERITY_ID\")\n",
    "\n",
    "#MLR With all Features R2 = 0.18966380192691812\n",
    "#MLR with Variance Threshold used R2 = 0.014341587255520503\n",
    "#MLR with Select K Best with top 9 R2 = 0.17866264279980612\n",
    "#MLR with Select K Best with top 20 R2 = 0.18685250796314865\n",
    "#MLR with fs select liner (L1 based)R2 (30 features)= 0.1893704622435573"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf.shape\n",
    "\n",
    "ldf.columns\n",
    "#30 features were kept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree Based Recursive Selectionn:\n",
    "# Let's perform a similar technique using tree-based modeling\n",
    "\n",
    "def fs_select_trees(df, label=\"\"):\n",
    "  from sklearn.ensemble import ExtraTreesClassifier\n",
    "  from sklearn.feature_selection import SelectFromModel\n",
    "  import pandas as pd\n",
    "\n",
    "  X = df.drop(columns=[label])\n",
    "  y = df[label]\n",
    "\n",
    "  clf = ExtraTreesClassifier(n_estimators=50)\n",
    "  clf = clf.fit(X, y)\n",
    "  sel = SelectFromModel(clf, prefit=True)\n",
    "  sel.transform(X)\n",
    "\n",
    "  columns = list(X.columns[sel.get_support()])\n",
    "  columns.append(label)\n",
    "  return df[columns]\n",
    "\n",
    "# Integrate this new function into the pipeline:\n",
    "tdf = get_data('CuperCut_TrainCrash.csv', ['CRASH_ID'])\n",
    "tdf = bin_groups(tdf)\n",
    "tdf = drop_columns_missing_50(tdf)\n",
    "tdf = impute_mean(tdf)\n",
    "tdf = fs_select_trees(tdf, label=\"CRASH_SEVERITY_ID\")\n",
    "model = fit_crossvalidate_mlr(tdf, 10, \"CRASH_SEVERITY_ID\")\n",
    "\n",
    "#MLR With all Features R2 = 0.18966380192691812 --> original\n",
    "#MLR with Variance Threshold used R2 = 0.014341587255520503\n",
    "#MLR with Select K Best with top 9 R2 = 0.17866264279980612\n",
    "#MLR with Select K Best with top 20 R2 = 0.18685250796314865\n",
    "#MLR with fs select liner R2 (30 features)= 0.1893704622435573 --> Best one we are going with to select features\n",
    "#MLR with Tree Recursive Selection = -0.0004624694798549167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying the Model:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
